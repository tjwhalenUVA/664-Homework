---
title: 'SYST 664: HW Assignment 5'
author: "Jake Whalen"
date: "February 26, 2018"
output:
  word_document:
    pandoc_args: --toc
---

```{r chunkOpts, echo=FALSE}
knitr::opts_chunk$set(echo = F, 
                      warning = F, 
                      fig.width=6, 
                      fig.height=3, 
                      comment='')
```

```{r setup}
source("05_setup.R")
```

```{r p1}
source("05_problem_01.R")
```

```{r p2}
source("05_problem_02.R")
```

# Problem 1

In previous years, students in this course collected data on people’s preferences in the two Allais gambles from Assignment 2.
For this problem, we will assume that responses are independent and identically distributed, and the probability is $\pi$ that a person’s response is B in the first gamble and C in the second gamble.

### Part A

Assume that the prior distribution for $\pi$ is Beta(1, 3).
Below is a plot of a Beta(1, 3) distribution.

```{r}
p1.a.plot
```

The table below shows the prior mean and standard deviation for $\pi$ as well as a 95% symmetric tail area credible interval for the prior probability that a person would choose B and C.

```{r}
knitr::kable(p1.a.df)
```

I do not think this is a reasonable prior distribution to use for this problem.
I think that the prior distribution should have a higher mean.
More people would be inclined to take the certainty of B in gamble 1 specifically.
In gamble 2 the difference in winning nothing in C and D is 0.01.
Therefore most people would be inclined to choose C because of the possibility of winning $100 more then in D.

### Part B

In 2009, 19 out of 47 respondents chose B and C.
The plot below shows the posterior distribution for the probability that a person in this population would choose B and C.

```{r}
p1.b.plot.stats
```

The table below shows the posterior mean and standard deviation, and a 95% symmetric tail area credible interval for the posterior probability that a person in this population would choose B and C.

```{r}
knitr::kable(p1.b.df)
```

Below is a triplot of the prior, posterior and normalized likelihood.

```{r}
p1.b.plot.tri
```

### Part C

In 2011 another 47 responses were collected.
This time, 20 out of 47 people said they preferred B and C.
Assume that the Spring 2011 respondents have the same distribution of responses as the Spring 2009 respondents.
Repeat part b, using the posterior distribution from the 2009 sample as your prior distribution.
The plot below shows the posterior distribution for the probability that a person in this population would choose B and C.

```{r}
p1.c.plot.stats
```

The table below shows the posterior mean and standard deviation, and a 95% symmetric tail area credible interval for the posterior probability that a person in this population would choose B and C.

```{r}
knitr::kable(p1.c.df)
```

Below is a triplot of the prior, posterior and normalized likelihood.

```{r}
p1.c.plot.tri
```

### Part D

The plot below shows the distributions generated in the three previous parts of this problem.

```{r}
p1.d.plot
```

We started out with the Beta(1, 3) prior distribution from part A (Prior.A).
Then we calculated the posterior after having collected more data on peoples preferences in 2009 (Posterior.2009).
Then again in 2011 more data was collected and the posterior distribution was calculated again (Posterior.2011).

The table below shows the means, standard deviation, median, and 95% credible intervals for each of the distributions.

```{r}
knitr::kable(p1.final.df)
```

The prior has a very low mean and median when compared to the posteriors.
The prior is also very spread out compared to the posteriors.
The posteriors ahow the distribution mean centering in on 0.4.
The spread of the distributions are also decreasing as more data is collected.

# Problem 2
This problem concerns the automobile data from Assignments 3 and 4.

### Part A
As in Assignment 4, assume that counts of cars per 15-second interval are independent and identically distributed Poisson random variables with unknown mean $\Lambda$.
Assume a uniform prior distribution for $\Lambda$.
(As for Assignment 4, you can approximate this prior distribution by using a Gamma distribution with shape 1 and scale 10,000.)
The car counts for the first 10 time intervals are 2, 2, 1, 1, 0, 4, 3, 0, 2, 1.
Find the posterior distribution for $\Lambda$ conditional on the first 10 observations.
Plot the posterior density function for $\Lambda$ conditional on the first 10 observations.
Find a 90% 2-sided posterior credible interval for $\Lambda$.

```{r}
p2.a.posterior
```

* 90% 2-sided posterior credible interval for $\Lambda$
    + Upper: `r round(p2.a.lower, 3)`
    + Lower: `r round(p2.a.upper, 3)`


### Part B
Using the posterior distribution from Part a, find the predictive distribution for the total number of cars in the next 11 time periods.
The predictive distribution is a Negative Binomial.
Finding its parameters (size and prob) can be accomplished using the parameters from the posterior in part A.


* The resulting Values are:
    + $Size = Shape =$ `r p2.size.b`
    + $Prob = 1 / (1 + n * Scale) =$ `r p2.prob.b`

Plot the probability mass function for the predictive distribution.

```{r}
p2.b.pred.plot
```

### Part C
Find the posterior predictive probability that between 10 and 30 cars will pass by in the next 11 time periods.
This is done by summing the probabilities from 10 cars to 30 cars arriving in the distribution above.

```{r, echo=TRUE}
p2.b.df %>%
    filter(cars >= 10, 
           cars <= 30) %>%
    summarise(pred = sum(Prediction)) %>%
    .$pred
```

### Part D
If the number of cars passing during a single time period has a Poisson distribution with parameter $\Lambda$, then the number of cars passing during 11 time periods has a Poisson distribution with parameter 11$\Lambda$.
Use the mean of the posterior distribution as a point estimate of $\Lambda$, and use this point estimate to find a Poisson distribution for predicting the number of cars passing during the next 11 periods.
The point estimate is calculated first using the posterior parameters found in part B.

```{r, echo=TRUE}
p2.d.pointEst <- 11 * p2.a.shape * p2.a.scale
p2.d.pointEst
```

This estimate is used in the R function `dpois` to create the predictive distribution for the next 11 time periods.
The plot below shows the result of this approach.

```{r}
p2.d.pred.plot
```

The code below calculates the probability that between 10 and 30 cars pass in these 11 time periods.

```{r, echo=TRUE}
p2.d.dist.df %>%
    filter(cars >= 10, 
           cars <= 30) %>%
    summarise(post = sum(Posterior)) %>%
    .$post
```

### Part E

The plot below shows the distributions for Parts c and d.

```{r}
p2.e.pred.plot
```

The table below shows the probabilities of 10 to 30 cars passing in the respective period of intervals.

```{r}
knitr::kable(p2.e.probs)
```

The distribution from part C is more spread out and thus has a higher amount of probability below 10 and over 30.
This results in the lower probability as compared to the distribution from part D in the table.
By using the mean from part C as a point estimate for part D the predictive distribution in D reduced sread and became tighter around the mean.